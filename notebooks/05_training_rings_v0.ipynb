{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943aaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "repo_path = '/home/micael.verissimo/paper_lzt/exp-connection-hep/'\n",
    "sys.path.insert(0, repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fd39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 19:34:00.287630: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-09 19:34:00.287677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-09 19:34:00.288562: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-09 19:34:00.293916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 19:34:07.469736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import time \n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configure TensorFlow before importing other modules\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow warnings (optional)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, List, Callable, Dict, Union\n",
    "\n",
    "from src.callbacks import sp_index\n",
    "from src.constants import GeV, et_bins, eta_bins\n",
    "from src.decorators import Summary, Reference\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f2e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "Available devices:\n",
      "  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU memory growth enabled for 1 GPU(s)\n",
      "TensorFlow configuration completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 19:34:41.956313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:42.190267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:42.190465: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# Configure TensorFlow to avoid BLAS errors and handle GPU/CPU properly\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check for GPU availability and configure accordingly\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Configure GPU memory growth to avoid memory issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs detected, using CPU\")\n",
    "\n",
    "# Alternative: Force CPU usage if GPU issues persist\n",
    "# Uncomment the next lines if you want to force CPU usage\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "# print(\"Forcing CPU usage\")\n",
    "\n",
    "# Set number of threads for CPU operations\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all available cores\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all available cores\n",
    "\n",
    "print(\"TensorFlow configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a75e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "sgn_df = pd.concat([pd.read_parquet(os.path.join(repo_path, f'data/processed/{iname}')) for iname in ['zee_avg250_100k.parquet']], axis=0)\n",
    "bkg_df = pd.concat([pd.read_parquet(os.path.join(repo_path, f'data/processed/{iname}')) for iname in ['jf17_avg250_100k.parquet']], axis=0)\n",
    "\n",
    "m_df = pd.concat([sgn_df, bkg_df], axis=0)\n",
    "m_df = m_df.loc[m_df.cl_eta.abs() <= 2.5]\n",
    "m_df = m_df.loc[m_df.cl_et >= 15000]\n",
    "\n",
    "input_cols = [f\"cl_ring_{idx}\" for idx in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb648dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    121786\n",
       "0    113291\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559569a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing et bin = [15.0, 30[ and eta bin = [0.0, 0.8[\n",
      "Processing et bin = [15.0, 30[ and eta bin = [0.8, 1.37[\n",
      "Processing et bin = [15.0, 30[ and eta bin = [1.37, 1.54[\n",
      "Processing et bin = [15.0, 30[ and eta bin = [1.54, 2.37[\n",
      "Processing et bin = [15.0, 30[ and eta bin = [2.37, 2.5[\n",
      "Processing et bin = [30.0, 50.0[ and eta bin = [0.0, 0.8[\n",
      "Processing et bin = [30.0, 50.0[ and eta bin = [0.8, 1.37[\n",
      "Processing et bin = [30.0, 50.0[ and eta bin = [1.37, 1.54[\n",
      "Processing et bin = [30.0, 50.0[ and eta bin = [1.54, 2.37[\n",
      "Processing et bin = [30.0, 50.0[ and eta bin = [2.37, 2.5[\n",
      "Processing et bin = [50.0, inf[ and eta bin = [0.0, 0.8[\n",
      "Processing et bin = [50.0, inf[ and eta bin = [0.8, 1.37[\n",
      "Processing et bin = [50.0, inf[ and eta bin = [1.37, 1.54[\n",
      "Processing et bin = [50.0, inf[ and eta bin = [1.54, 2.37[\n",
      "Processing et bin = [50.0, inf[ and eta bin = [2.37, 2.5[\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tot = 0\n",
    "for iet, (l_iet, h_iet) in enumerate(et_bins):\n",
    "    for ieta, (l_ieta, h_ieta) in enumerate(eta_bins):\n",
    "        print(f'Processing et bin = [{l_iet}, {h_iet}[ and eta bin = [{l_ieta}, {h_ieta}[')\n",
    "        m_df.loc[(m_df.cl_et >= l_iet*GeV) & (m_df.cl_et < h_iet*GeV), 'et_bin'] = int(iet)\n",
    "        m_df.loc[(m_df.cl_eta.abs() >= l_ieta) & (m_df.cl_eta.abs() < h_ieta), 'eta_bin'] = int(ieta)\n",
    "        l_samples = len(m_df.loc[(m_df.et_bin == iet) & (m_df.eta_bin == ieta)])\n",
    "        tot += l_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40f9aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235077, 235077)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_df), tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c0b7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2.]), array([1., 2., 3., 0., 4.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.et_bin.unique(), m_df.eta_bin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1599003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm1(data):\n",
    "    norms = np.abs(data.sum(axis=1))\n",
    "    norms[norms == 0] = 1\n",
    "    return data / norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f58cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_mlp(input_shape: int, n_layers: int, n_units: int, seed: int=512) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a simple Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of input features for the model.\n",
    "        n_layers (int): The number of hidden layers in the MLP.\n",
    "        n_units (int): The number of neurons in each hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model with the specified architecture.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(input_shape,), name='Input')\n",
    "    for ilayer in range(n_layers):\n",
    "        dense = tf.keras.layers.Dense(n_units, activation='relu', name=f'dense_layer_{ilayer}',\n",
    "                                      kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed),\n",
    "                                      bias_initializer='zeros')(inputs if ilayer == 0 else dense)\n",
    "    # classification layer\n",
    "    dense = tf.keras.layers.Dense(1, activation='linear', name='output_for_inference',\n",
    "                                  kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed),\n",
    "                                  bias_initializer='zeros')(dense) \n",
    "    output = tf.keras.layers.Activation('sigmoid', name='output_for_training')(dense)\n",
    "    model = tf.keras.Model(inputs, output, name=\"model\")\n",
    "    return model\n",
    "\n",
    "def class_weight(target: np.ndarray) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate class weights and sample weights for binary classification.\n",
    "\n",
    "    Args:\n",
    "        target (np.ndarray): Array of binary target labels (e.g., 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series containing sample weights for each instance, \n",
    "                   with the column name 'weight'.\n",
    "    \"\"\"\n",
    "    classes = np.unique(target)\n",
    "    # [-1,1] or [0,1]\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=target)\n",
    "    class_weights = {cl: weights[idx] for idx, cl in enumerate(classes)}\n",
    "    sample_weight = np.ones_like(target, dtype=np.float32)\n",
    "    sample_weight[target == 1] = weights[1]\n",
    "    sample_weight[target != 1] = weights[0]\n",
    "    return pd.Series(sample_weight).to_frame('weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3588c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataframe: pd.DataFrame, input_cols: List[str], target_col: str,\n",
    "                n_folds: int=5,\n",
    "                n_epochs: int=100, \n",
    "                batch_size: int=1024, \n",
    "                seed: int=512,\n",
    "                optimizer: Any=None,  # Changed to None to create fresh optimizers\n",
    "                loss: Any='binary_crossentropy',\n",
    "                decorators: List=[],\n",
    "                patience: int=25,\n",
    "                verbose: bool=True,\n",
    "                save_path: str=os.path.join(repo_path, 'data/models')) -> None:\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    X, y = norm1(dataframe[input_cols].values), dataframe[target_col].values\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test   = X[test_index] , y[test_index]\n",
    "    \n",
    "        l_model = build_simple_mlp(input_shape=X.shape[1], n_layers=1, n_units=5, seed=np.random.randint(0, 1000))\n",
    "        \n",
    "        # Create a fresh optimizer instance for each model to avoid state conflicts\n",
    "        if optimizer is None:\n",
    "            # Use legacy Adam optimizer to avoid the variable recognition issue\n",
    "            fresh_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "        else:\n",
    "            # Create a new instance of the same optimizer type\n",
    "            if hasattr(optimizer, 'get_config'):\n",
    "                config = optimizer.get_config()\n",
    "                fresh_optimizer = optimizer.__class__.from_config(config)\n",
    "            else:\n",
    "                # Fallback to legacy Adam\n",
    "                fresh_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "        \n",
    "        l_model.compile(optimizer=fresh_optimizer, \n",
    "                    loss=loss, \n",
    "                    metrics=['accuracy']\n",
    "                    )\n",
    "        \n",
    "        sp_index_callback = sp_index(validation_data=(x_test, y_test),\n",
    "                                    patience=patience, verbose=False, save_the_best=True)\n",
    "        start = time.time()\n",
    "        history = l_model.fit(x_train, y_train, epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            validation_data=(x_test, y_test),\n",
    "            sample_weight=class_weight(y_train),\n",
    "            callbacks=[sp_index_callback],\n",
    "            shuffle=True\n",
    "        ).history\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        # Run decorators with output capture\n",
    "        et_bin = int(dataframe.et_bin.unique()[0])\n",
    "        eta_bin = int(dataframe.eta_bin.unique()[0])\n",
    "        \n",
    "        \n",
    "        for decorator in decorators:\n",
    "            decorator(history , {'model':l_model, 'data':(x_train, y_train),  'data_val':(x_test, y_test) })\n",
    "            \n",
    "        d = { \n",
    "            'history'          : history, \n",
    "            'model'            : json.loads(l_model.to_json()), \n",
    "            'weights'          : l_model.get_weights(),\n",
    "            'folds'            : i,\n",
    "            'model_type'       : 'mlp_ss',\n",
    "            'time_to_train'    : (end-start)}\n",
    "        output = os.path.join(save_path, \n",
    "                              f'ss_model_et{et_bin}_eta{eta_bin}_fold{i}.pkl')\n",
    "        pickle.dump(d, open(output, 'wb'))\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4bb92fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for et bin = 0 and eta bin = 0\n",
      "Training model for et bin = 0 and eta bin = 1\n",
      "Training model for et bin = 0 and eta bin = 2\n",
      "Training model for et bin = 0 and eta bin = 3\n",
      "Training model for et bin = 0 and eta bin = 4\n",
      "Training model for et bin = 1 and eta bin = 0\n",
      "Training model for et bin = 1 and eta bin = 1\n",
      "Training model for et bin = 1 and eta bin = 2\n",
      "Training model for et bin = 1 and eta bin = 3\n",
      "Training model for et bin = 1 and eta bin = 4\n",
      "Training model for et bin = 2 and eta bin = 0\n",
      "Training model for et bin = 2 and eta bin = 1\n",
      "Training model for et bin = 2 and eta bin = 2\n",
      "Training model for et bin = 2 and eta bin = 3\n",
      "Training model for et bin = 2 and eta bin = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 19:34:44.839411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.839628: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.839758: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.887041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.887234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.887381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-09 19:34:44.887477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9797 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:00:10.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "for iet, ieta in itertools.product(range(3), range(5)):\n",
    "        \n",
    "    with open(os.path.join(repo_path, f'data/processed/references_et{iet}_eta{ieta}.json'), 'r') as f:\n",
    "        ref = json.load(f)\n",
    "    \n",
    "    decorators = [Summary(detailed=True, verbose=False), Reference(ref, verbose=False)]\n",
    "    l_data = m_df.loc[(m_df.et_bin == iet) & (m_df.eta_bin == ieta)]\n",
    "    print(f'Training model for et bin = {iet} and eta bin = {ieta}')\n",
    "    train_model(l_data, input_cols=input_cols, target_col='target',\n",
    "                n_folds=5,\n",
    "                n_epochs=1000,\n",
    "                batch_size=512,\n",
    "                seed=512,\n",
    "                optimizer=None,  # Changed to None to use legacy Adam optimizer\n",
    "                loss='binary_crossentropy',\n",
    "                decorators=decorators,\n",
    "                verbose=False,\n",
    "                patience=25,\n",
    "                save_path=os.path.join(repo_path, 'data/models/rings_v0_models'))\n",
    "    tf.keras.backend.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f7aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
